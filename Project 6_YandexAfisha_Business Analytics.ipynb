{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUSINESS ANALYTICS\n",
    "\n",
    "Project description.\n",
    "\n",
    "Internship in the analytical department at Yandex.Afisha.\n",
    "\n",
    "The task is to help optimize marketing expenses.\n",
    "\n",
    "We have data from server logs with data on Yandex.Afisha visits from June 2017 through May 2018, dump file with all orders for the period, marketing expenses statistics.\n",
    "\n",
    "We should find out\n",
    "1. How people use the product\n",
    "2. When the customer starts to buy\n",
    "3. How much money each customer brings\n",
    "4. When the customer pay off\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Download the data and prepare it for analysis\n",
    "\n",
    "Store the data on visits, orders, and expenses in variables, optimizing the data for analysis and changing columns with the incorrect data type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits= pd.read_csv('/datasets/visits_log_us.csv',dtype={'Device': 'category'},\n",
    "    parse_dates=['Start Ts', 'End Ts'])\n",
    "orders=pd.read_csv('/datasets/orders_log_us.csv', parse_dates=['Buy Ts'])\n",
    "costs=pd.read_csv('/datasets/costs_us.csv', parse_dates=['dt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits.rename(columns={'Source Id':'source_id'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of the data\n",
    "\n",
    "The visits table (server logs with data on website visits):\n",
    "Uid — user's unique identifier\n",
    "Device — user's device\n",
    "Start Ts — session start date and time\n",
    "End Ts — session end date and time\n",
    "Source Id — identifier of the ad source the user came from\n",
    "All dates in this table are in YYYY-MM-DD format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The orders table (data on orders):\n",
    "Uid — unique identifier of the user making an order\n",
    "Buy Ts — order date and time\n",
    "Revenue — Yandex.Afisha's revenue from the order\n",
    "The costs table (data on marketing expenses):\n",
    "source_id — ad source identifier\n",
    "dt — date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.rename(columns={'Buy Ts':'buy_ts', 'Revenue':'revenue'}, inplace=True)\n",
    "orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "costs — expenses on this ad source on this day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "costs['source_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Make reports and calculate metrics.\n",
    "\n",
    "1. Product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1.How many people use it every day, week, and month.\n",
    "2.Sessions per day\n",
    "3.Length of each session.\n",
    "4.User retention rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.Let's look at the data on user activity. This metric tells how loyal the audience is — how often they return.\n",
    "#DAU — the number of daily active (unique) users, \n",
    "#WAU — the number of weekly active users, \n",
    "#MAU — the number of monthly active users\n",
    "\n",
    "#To calculate daily, weekly and monthly activity, we'll first create separate columns for year, month, and week values.\n",
    "visits['date'] = pd.to_datetime(visits['Start Ts']).dt.date\n",
    "visits['month'] = visits['Start Ts'].astype('datetime64[M]')\n",
    "visits['week']  = visits['Start Ts'].astype('datetime64[W]')\n",
    "orders['month'] = orders['buy_ts'].astype('datetime64[M]')\n",
    "orders['week'] = orders['buy_ts'].astype('datetime64[W]')\n",
    "\n",
    "visits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's calculate metrics. We'll group the data by session date/week/month and find the means:\n",
    "dau=visits.groupby(visits['date'])['Uid'].nunique().reset_index()\n",
    "#Calculating dau\n",
    "dau_total = visits.groupby('date').agg({'Uid':'nunique'}).mean()\n",
    "\n",
    "#Calculating wau\n",
    "wau_total = visits.groupby('week').agg({'Uid':'nunique'}).mean()\n",
    "\n",
    "#Calculating mau\n",
    "mau_total = visits.groupby('month').agg({'Uid':'nunique'}).mean()\n",
    "\n",
    "#Calculating sticky factors\n",
    "sf_w = dau_total / wau_total\n",
    "sf_m = dau_total / mau_total\n",
    "#Printing the results\n",
    "print('Average number of daily users:', int(dau_total))\n",
    "print('Average number of weekly users:',int(wau_total), 'sticky factor:', round(float(sf_w),2))\n",
    "print('Average number of monthly users:',int(mau_total), 'sticky factor:', round(float(sf_m),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in average there are 907 users who use the product every day. Only 16% of the users who have used the product at least once a week, continue to do it every day, and only 4% of the users who have used the product at least once a month, continue to use it every day. 25% of users have used the service once a month us it weekly. Not so many users like to use the product regularly, may be we need to find the way how to involve more people to our product.\n",
    "Let' look how dau, wau and mau change in time. We will plot line graph and mark the mean value to see the dynamic of daily, weekly and monthly users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = px.line(dau,x=\"date\", y=\"Uid\",title='DAU')\n",
    "#adding reference line with average DAU over time\n",
    "fig.add_hline(y=dau['Uid'].mean(),line_dash=\"dash\", line_color=\"purple\", annotation_text=\"average DAU\",\n",
    "             annotation_position=\"top left\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAU is stable. There is sudden peak in one day the dau is 3319, but it was only one day, after that dau returned to its original value.\n",
    "In the same time we see that wau changes in time significantly. We have increas from 2000 users in August 2017 to 7800 users in October 2017. And we have this level stable till April 2018, after that date dau began to decrease. That means that our sticky factor is falling down.Less users used the product daily. For wau we have obviously the same peak of 10586 users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wau=visits.groupby(visits['week'])['Uid'].nunique().reset_index()\n",
    "wau.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = go.Figure(layout=go.Layout(\n",
    "        title=go.layout.Title(text=\"WAU\")))\n",
    "fig.add_trace(go.Scatter(x=wau['week'], \n",
    "                        y=wau['Uid'],\n",
    "                    mode='lines+markers',\n",
    "                    name='lines+markers',line = dict(color='purple', width=5)))\n",
    "fig.add_hline(y=wau['Uid'].mean(),line_dash=\"dash\", line_color=\"red\", annotation_text=\"average WAU\",\n",
    "             annotation_position=\"top left\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mau=visits.groupby(visits['month'])['Uid'].nunique().reset_index()\n",
    "mau.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mau['Uid'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = go.Figure(layout=go.Layout(\n",
    "        title=go.layout.Title(text=\"MAU\")))\n",
    "fig.add_trace(go.Scatter(x=mau['month'], \n",
    "                        y=mau['Uid'],\n",
    "                    mode='lines+markers',\n",
    "                    name='lines+markers',line = dict(color='purple', width=5)))\n",
    "fig.add_hline(y=mau['Uid'].mean(),line_dash=\"dash\", line_color=\"red\", annotation_text=\"average MAU\",\n",
    "             annotation_position=\"top left\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same dynamic and correlation between WAU and MAU with periods when MAU increases more than WAU (periods when users comes to the site with less regularly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.How many sessions are there per day? (One user might have more than one session.) Let's find number of sessions per each user. We can count the sessions and then we can divide number of sessions on users.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the session table\n",
    "sessions = visits.groupby('date').agg({'Uid':['count', 'nunique']})\n",
    "\n",
    "#Renaiming the columns\n",
    "sessions.columns = ['n_sess', 'n_user']\n",
    "sessions['sess_per_user'] = sessions.n_sess / sessions.n_user\n",
    "\n",
    "#Plotting line graph to see the dynamic of session per user\n",
    "fig, ax = plt.subplots(figsize = (10,6))\n",
    "ax.grid(axis ='y')\n",
    "ax.plot(sessions.index, sessions['sess_per_user'], label = 'Sessions per user')\n",
    "\n",
    "#Adding h-line with mean number of session per user\n",
    "plt.axhline(y=sessions['sess_per_user'].mean(), linewidth=1, color = 'blue', alpha = 0.7)\n",
    "mean_sess_per_user = sessions['sess_per_user'].mean()\n",
    "ax.annotate(f\"{mean_sess_per_user:.2f}\", \n",
    "             xy=(sessions.index[0], mean_sess_per_user), xytext=(-30, 5), \n",
    "             textcoords=\"offset points\", ha=\"right\", va=\"top\", color='blue', fontsize=10)\n",
    "\n",
    "\n",
    "#Adding additional info\n",
    "ax.legend()\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Number of sessions')\n",
    "ax.set_title('Number of sessions per user')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users open the site just once a day. Number of session is stable throughout all the period. However there are one high peak 1.23 sessions per user and one low peak with 1.0 session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.Length of each session. Average session length, or ASL, is the amount of time users spend with a product in the average session.\n",
    "visits['session_duration_sec'] = (visits['End Ts'] - visits['Start Ts']).dt.seconds\n",
    "print(visits['session_duration_sec'].mean()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's have a look at the distribution\n",
    "visits['session_duration_sec'].hist(bins=50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When the distribution is normal or close, it's correct to use the mean or median. But in our case it's impossible, so we have to calculate the mode:\n",
    "asl=visits['session_duration_sec'].mode()\n",
    "#Printing the result\n",
    "print('The average  daily session length per user (ASL) is:', int(asl), 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.User's retention rate. \n",
    "\n",
    "The retention rate tells how many users from a cohort have remained active compared to their initial number.\n",
    "\n",
    "Basically, for retention we need to find the differnce between any session and first session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find first activity date for each user and join it into the visits dataset\n",
    "first_activity_date=visits.groupby('Uid')['date'].min()\n",
    "first_activity_date.name='first_activity_date'\n",
    "visits=visits.join(first_activity_date, on ='Uid')\n",
    "#Check\n",
    "visits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sett the correct type of `first_activity_date` column\n",
    "\n",
    "visits['first_activity_date']= pd.to_datetime(visits['first_activity_date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add `first_activity_month` column\n",
    "\n",
    "visits['first_activity_month']=(visits.first_activity_date - pd.to_timedelta(\n",
    "    visits.first_activity_date.dt.day, unit='d')+ timedelta(days=1)).dt.date\n",
    "#Check\n",
    "visits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding `activity_month` column\n",
    "\n",
    "visits['date']=pd.to_datetime(visits['date'])\n",
    "visits['activity_month']=(visits.date-pd.to_timedelta(\n",
    "    visits.date.dt.day, unit='d')+timedelta(days=1)).dt.date\n",
    "visits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate cohort lifetime\n",
    "visits['cohort_lifetime']=((visits['activity_month']-visits['first_activity_month'])/np.timedelta64(1,'M')).round().astype(int)\n",
    "visits.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can group the visits table with first activity month and cohort lifetime and calculate the number of unique users in each lifetime week for each cohort.\n",
    "After we divide the number of users in lifetime month to the number of users in first activity month and get the Retention rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create cohort table\n",
    "cohorts=visits.groupby(['first_activity_month', 'cohort_lifetime']).agg({'Uid':'nunique'}).reset_index()\n",
    "#Initial number of users for the cohort\n",
    "initial_user_count=cohorts[cohorts['cohort_lifetime']==0][['first_activity_month', 'Uid']]\n",
    "#Rename the column\n",
    "initial_user_count=initial_user_count.rename(columns={'Uid':'cohort_users'})\n",
    "#Merge with cohort users\n",
    "cohorts=cohorts.merge(initial_user_count, on='first_activity_month')\n",
    "\n",
    "cohorts.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add retention rate column\n",
    "cohorts['retention']=cohorts['Uid']/cohorts['cohort_users']\n",
    "#Retention pivot table\n",
    "retention= cohorts.pivot_table(\n",
    "    index='first_activity_month',\n",
    "    columns='cohort_lifetime',\n",
    "    values='retention',\n",
    "    aggfunc='sum'\n",
    ")\n",
    "retention.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the gradient heatmap of the retention rates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='dark')\n",
    "plt.figure(figsize=(11, 7))# Set the figure size\n",
    "plt.title('Retention rate')# Set the visualization title\n",
    "sns.heatmap(\n",
    "    retention,\n",
    "    annot=True,\n",
    "    fmt='.1%',\n",
    "    linewidths=1,\n",
    "    linecolor='red',\n",
    "    vmax=0.1\n",
    ") # Make a heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retention rate is small, less then 10% of users continue using the product after the first month.\n",
    "\n",
    "The most successful cohort is the first one June 2017 Cohort. They have 7.9% of users who continue to use the site in the next month after the first usage and the have that rate for the next 6 months. Then after 6 months retention rate begin to drop to 4.5% in May 2018. \n",
    "\n",
    "The September 2017 Cohort gas the highest retention rate in the first month 8.5%, but it dropped rapidly in 3 months less than 4%. We see also that from December 2017 and in 2018 retention rate begins to drop for those cohorts below 6% in the first lifetime month.\n",
    "\n",
    "We have 907 users, who use the product every day in average. \n",
    "\n",
    "Average number of weekly users - 5716\n",
    "\n",
    "Average number of monthly users - 23228. \n",
    "\n",
    "Only 16% of the users who have used the product at least once a week, continue to do it every day, and only 4% who have used the product at least once a month, continue to use it every day. \n",
    "\n",
    "25% of users who have used the site once a month use it weekly. We see that WAU changes in time significantly. We have increase from 2000 users in August 2017 to 7800 users in October 2017. And we have this level stable till April 2018, after that DAU began to decrease.\n",
    "\n",
    "That means that our sticky factor is falling down. In average it is 16%, but for the period form October 2017 till April 2018 it is just 12%.\n",
    "\n",
    "Most often users spend just 1 minute using the product, but the half of the total number of users spend more than 6 minutes during the one session.\n",
    "\n",
    "Less then 10% of users continue using the product after the first month when they began to use it. \n",
    "\n",
    "The September Cohort has the highest retention rate in the first month (8.5%) but it dropped at 3 months to 4%. The most successful cohort is the June 2017 Cohort. \n",
    "\n",
    "From December 2017 to 2018 retention rate begins to drop for those cohorts to 6% in the first lifetime month. My be there is a influence by the seasonal factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When do people start buying? Conversion depicts how long does it take from first visit to the purchase in days.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Let's groupby on orders table in order to find the first purchase for each user. First order df with users that've made purchases and the date of their first order.\n",
    "\n",
    "2.Find the first visit for each user. \n",
    "\n",
    "3.Merge two dataframes together: user,first visit, first order.\n",
    "\n",
    "4.Find the difference in days between the first order and first visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find first purchase date for each user\n",
    "first_purchase_date = orders.groupby('Uid')['buy_ts'].min()\n",
    "#Renaming the column\n",
    "first_purchase_date.name = 'first_purchase_date'\n",
    "\n",
    "#Add the `first_purchase` column to orders table\n",
    "orders = orders.join(first_purchase_date, on = 'Uid')\n",
    "\n",
    "#Check\n",
    "orders.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the `first_activity_date` column to orders table\n",
    "orders = orders.merge(first_activity_date, on = 'Uid')\n",
    "#Adjusting format of `first_activity_date` column\n",
    "orders['first_activity_date'] = pd.to_datetime(orders['first_activity_date'])\n",
    "\n",
    "#Create `purchase_time` column\n",
    "orders['purchase_time'] = (\n",
    "    (orders.first_purchase_date - orders.first_activity_date)/np.timedelta64(1, 'D')).astype('int')\n",
    "\n",
    "#Plot a histogram,\n",
    "orders['purchase_time'].hist(bins= 30)\n",
    "plt.title('Conversion')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Number of users')\n",
    "plt.show()\n",
    "print('Median conversion value is', orders['purchase_time'].median(), 'Days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders['purchase_time'].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that people usually order something in the same day. 75% of people make the first order within 4 days. Now let's find how many people convert at all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many visits converted\n",
    "print(' The overall conversion is {:.1%}'.format(orders['Uid'].nunique()/visits['Uid'].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any way, at least 16% of users made  one order or more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.How many orders do they make during a given period of time?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining cohort month through first purchase\n",
    "#Creating `first_buy_month`  and `buy_month` columns\n",
    "orders['first_buy_month'] = (orders.first_purchase_date - pd.to_timedelta(\n",
    "    orders.first_purchase_date.dt.day, unit='d') + timedelta(days=1)).dt.date\n",
    "orders['buy_month'] = (orders.buy_ts - pd.to_timedelta(\n",
    "    orders.buy_ts.dt.day, unit='d') + timedelta(days=1)).dt.date\n",
    "#Creating `cohort_lifetime`column\n",
    "orders['cohort_lifetime'] = ((\n",
    "    orders['buy_month'] - orders['first_buy_month']) / np.timedelta64(1,'M')).round().astype('int')\n",
    "   \n",
    "#Check\n",
    "orders.sample(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First_order_month now defines our cohort. Now let's aggregate the data : for each cohort we will find the month of purchase (then age from that), the number of purchases and unique customers. Calculate orders_per_buyer for each cohort and each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's define cohort size. this is one of the most important aggregation that we will use in cohort analysis\n",
    "#using the month of first purchase we will define which cohort buyer belongs to\n",
    "cohorts_buy = orders.groupby(['first_buy_month', 'cohort_lifetime']).agg({'Uid':['count','nunique']}).reset_index()\n",
    "cohorts_buy.columns = ['first_buy_month', 'cohort_lifetime', 'n_orders', 'n_buyers']\n",
    "#Calculating the initial number of users for each cohort\n",
    "init_uid_buy = cohorts_buy[cohorts_buy['cohort_lifetime']==0][['first_buy_month', 'n_buyers']]\n",
    "#Renaming the column for initial number of cohort buyers\n",
    "init_uid_buy=init_uid_buy.rename(columns={'n_buyers':'cohort_count'})\n",
    "\n",
    "#Adding the column `cohort_count` and calculating `orders_per_buer` in cohorts_buy table\n",
    "cohorts_buy = cohorts_buy.merge(init_uid_buy, on='first_buy_month')\n",
    "cohorts_buy['orders_per_buyer'] = cohorts_buy.n_orders / cohorts_buy.cohort_count\n",
    "\n",
    "#Creating pivot\n",
    "cohorts_buy_pivot = cohorts_buy.pivot_table(\n",
    "    index='first_buy_month', columns='cohort_lifetime', values = 'orders_per_buyer', aggfunc='sum')\n",
    "\n",
    "#Checking the result\n",
    "cohorts_buy_pivot.round(2).fillna('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#which cohort will be outrunning the others in term of cumulative purchases per user.\n",
    "#pivot\n",
    "cohorts_buy_pivot = cohorts_buy.pivot_table(\n",
    "    index='first_buy_month', columns='cohort_lifetime', values = 'orders_per_buyer', aggfunc='sum').cumsum(axis=1)\n",
    "\n",
    "#Check\n",
    "cohorts_buy_pivot.round(2).fillna('-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our first cohort has the biggest number of orders per user per month. 2.19 orders per user in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.What is the average purchase size?\n",
    "We can make a pivot table to find this value.\n",
    "We create a pivot and plot a graph for total mean purchase size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating average order size\n",
    "order_size = orders.pivot_table(\n",
    "    index='first_buy_month', columns = 'cohort_lifetime', values = 'revenue', aggfunc='mean')\n",
    "\n",
    "#Adding the totals\n",
    "order_size.loc['Mean'] = order_size.mean(numeric_only=True, skipna=True, axis=0)\n",
    "order_size = pd.concat([order_size, order_size.mean(numeric_only=True, skipna=True, axis=1).rename('Mean')], axis=1)\n",
    "\n",
    "#Printing the result\n",
    "order_size.round(2).fillna('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Revenue distribution by month:\n",
    "revenue_dist = orders.groupby('buy_month')['revenue'].mean().reset_index()\n",
    "\n",
    "#Line graph \n",
    "fig, ax = plt.subplots(figsize=(14,10))\n",
    "ax.plot(revenue_dist['revenue'])\n",
    "ax.grid(axis='y')\n",
    "\n",
    "#h-line\n",
    "revenue_mean = orders.revenue.mean()\n",
    "plt.axhline(y=revenue_mean, linewidth=3, color = 'red', alpha = 0.3)\n",
    "ax.text(revenue_dist['revenue'].iloc[0], revenue_mean-0.25, f\"Mean: {revenue_mean:.0f}\", color='red')\n",
    "\n",
    "#graph labels\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Dollar')\n",
    "plt.suptitle('Mean revenue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> On historical data, we see the following picture of the change in average revenue on a monthly basis:\n",
    "\n",
    "Firstly the average revenue increases and varies in a small range around the mean( about 5 USD), but tends to decrease towards the end of the study period.</b> <a class=\"tocSkip\"></a>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.How much money do they bring? (LTV)\n",
    "\n",
    "When a customer buys for the first time, we don't know if he will repeat the purchase and become a regular, or immediately leave. \n",
    "If you know the entire customer journey, then you can rationally distribute the budget: do not spend it on disloyal customers and invest more in those who stay for a long time. Lifetime Value (LTV) helps to predict this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the revenue per cohort in each month\n",
    "ltv_cohort=orders.groupby(['first_buy_month','month'])['revenue'].sum().reset_index()\n",
    "ltv_cohort=ltv_cohort.merge(init_uid_buy, on='first_buy_month')\n",
    "ltv_cohort['first_buy_month']=ltv_cohort['first_buy_month'].astype('datetime64[M]')\n",
    "ltv_cohort['age']=((ltv_cohort['month'] - ltv_cohort['first_buy_month']) / np.timedelta64(1,'M')).round()\n",
    "#Calculating the evarage revenue per user\n",
    "ltv_cohort['ltv'] = ltv_cohort.revenue / ltv_cohort.cohort_count\n",
    "ltv_cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltv_cohort_piv=ltv_cohort.pivot_table(\n",
    "    index='first_buy_month', \n",
    "    columns='age', \n",
    "    values='ltv', \n",
    "    aggfunc='sum'\n",
    ").cumsum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating LTV pivot table\n",
    "ltv_cohort_piv.index=ltv_cohort_piv.index.astype(str)\n",
    "\n",
    "\n",
    "#Plotting heat map\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "sns.heatmap(ltv_cohort_piv, annot=True, fmt='.2f', linewidths=1, linecolor='grey', cbar_kws= {'orientation': 'vertical'} \n",
    "            ).set(title ='LTV')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the best is September 2017 cohort. Each user from this cohort gives us 13.44 usd. \n",
    "June 2017 Cohort has also good with 11.88 usd. Users from other cohorts brings less than 9 usd. The worst result has the new fresh cohort of June 2018.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Marketing. Reports and metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How much money was spent? Overall, per source and over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's find total costs.\n",
    "print('Total marketing cost is {} usd'.format(costs['costs'].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a graph showing marketing costs of different ad sources\n",
    "#Group costs\n",
    "costs_d = costs.groupby('source_id')['costs'].sum().reset_index()\n",
    "\n",
    "#Plot a bar\n",
    "costs_d.plot(kind = 'bar', x = 'source_id', y = 'costs', figsize = (9,5)).grid(axis = 'y')\n",
    "plt.ylabel('USD')\n",
    "plt.title('Costs per different ad sources, USD')\n",
    "plt.show()\n",
    "\n",
    "#Print sorted costs\n",
    "costs_d.sort_values('costs', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We spend 141 321 usd for acquisition through the source 3. Now let's build a graph showing costs of different sources spent through time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Groupcosts with `source` column\n",
    "costs_source = costs.groupby(['source_id', 'dt'])['costs'].sum().reset_index()\n",
    "\n",
    "#Plot linegraph for the most expansive source\n",
    "plt.figure(figsize = (15,10))\n",
    "ax = sns.lineplot(\n",
    "        data=costs_source.query('source_id==3'), \n",
    "        x='dt', y='costs', hue='source_id', linewidth=1,\n",
    "        palette=sns.color_palette(\"bright\", 1)\n",
    "     )\n",
    "ax.grid(axis='y')\n",
    "ax.set_title('Costs Sorce N3')\n",
    "ax.set_xlabel('Months')\n",
    "ax.set_ylabel('Dollars')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot other sources\n",
    "plt.figure(figsize = (15,10))\n",
    "ax = sns.lineplot(\n",
    "        data=costs_source.query('source_id!=3'), \n",
    "        x='dt', y='costs', hue='source_id', linewidth=1,\n",
    "        palette=sns.color_palette(\"bright\", 6)\n",
    "     )\n",
    "ax.grid(axis='y')\n",
    "ax.set_title('Other sources')\n",
    "ax.set_xlabel('Months')\n",
    "ax.set_ylabel('Dollars')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#costs over time\n",
    "costs.set_index('dt', inplace=True)\n",
    "costs_month = costs['costs'].resample('M').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Plot the bar graph\n",
    "costs_month.index = costs_month.index.strftime('%y-%m')\n",
    "costs_month.plot(kind = 'bar', x = 'dt', y = 'costs', figsize = (10,6)).grid(axis = 'y')\n",
    "plt.ylabel('USD')\n",
    "plt.title('Costs over time, USD')\n",
    "plt.xlabel('Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it seen on the graph the marketing cost grow up from August 2017 untill the December of 2017. In January 2018 them began to decrease. \n",
    "The retention rate began to increase in that period of time synchronously, obviously as a result of spending more money on user's acquisition. After the decreasing of costs in January 2018 the retention rate also decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How much did customer acquisition from each of the sources cost?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = costs.reset_index()\n",
    "costs['dt_month'] = (costs['dt'] - pd.to_timedelta(costs['dt'].dt.day, unit='d') + timedelta(days=1)).dt.date\n",
    "\n",
    "#Group costs\n",
    "costs_month = costs.groupby('dt_month')['costs'].sum().reset_index()\n",
    "\n",
    "#Check\n",
    "costs_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#buyers_month table\n",
    "buyers_month = orders.groupby('first_buy_month')['Uid'].nunique().reset_index()\n",
    "#Rename the columns\n",
    "buyers_month.columns = ['dt_month', 'n_buyers']\n",
    "#Check\n",
    "buyers_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the both tables\n",
    "costs_month = costs_month.merge(buyers_month, on='dt_month')\n",
    "#Calculating CAC\n",
    "costs_month['cac'] = costs_month.costs / costs_month.n_buyers\n",
    "costs_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the graph\n",
    "plt.figure(figsize = (10,6))\n",
    "ax = sns.lineplot(data=costs_month, x='dt_month', y='cac', linewidth=1, palette=sns.color_palette(\"bright\", 1))\n",
    "ax.grid(axis='y')\n",
    "\n",
    "#mean CAC\n",
    "cac_mean = float(costs_month['cac'].mean())\n",
    "#Plot line with the mean CAC\n",
    "plt.axhline(y=cac_mean, linewidth=1, color = 'blue', alpha = 0.7)\n",
    "ax.text(costs_month['dt_month'].iloc[0], cac_mean +0.05, f\"Mean CAC: {cac_mean:.0f}\", color='blue', va='center')\n",
    "\n",
    "#Plot info\n",
    "ax.set_title('Costs of user acquisition in time')\n",
    "ax.set_xlabel('Months')\n",
    "ax.set_ylabel('USD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>\n",
    "The Mean CAC per user is about 9 USD. This value changes significantly over time. For example, it reaches a maximum value in August 2017 of more than 10 and a minimum in May 2018 of more than 7. In general, acquisition seems prohibitively expensive, given that Yandex itself is engaged in contextual advertising. We need to think about properly built SEO optimization of this site, although this is beyond the scope of our study, since the sources of attraction for us are marked only with numbers.\n",
    "</b> <a class=\"tocSkip\"></a>\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How much did customer acquisition from each of the sources cost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAC is the cost of customer acquisition. To calculate CAC, we need to divide marketing costs by the number of customers attracted. Let's calculate CAC for different channels of ad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table with the first source for each user\n",
    "user_source  = visits.sort_values(['Start Ts']).groupby('Uid')['source_id'].first().reset_index()\n",
    "\n",
    "#Merge user_sorce to orders table\n",
    "orders = orders.merge(user_source, on = 'Uid')\n",
    "\n",
    "#Group orders table with buy_date and source_id to find number of users per each source\n",
    "buyers_month_source = orders.groupby(['first_buy_month', 'source_id'])['Uid'].nunique().reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost_month_source table\n",
    "cost_month_source = costs.groupby(['dt_month', 'source_id'])['costs'].sum().reset_index()\n",
    "\n",
    "#Rename columns to merge\n",
    "buyers_month_source.rename(columns={'first_buy_month':'dt_month', 'Uid': 'n_buyers'}, inplace=True)\n",
    "#Merge\n",
    "cost_month_source = cost_month_source.merge(\n",
    "    buyers_month_source, how='left', left_on = ['dt_month', 'source_id'], right_on = ['dt_month', 'source_id'])\n",
    "#CAC per source\n",
    "cost_month_source['cac_source'] = cost_month_source.costs / cost_month_source.n_buyers\n",
    "\n",
    "#Check\n",
    "cost_month_source.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot line graph\n",
    "plt.figure(figsize = (10,6))\n",
    "ax = sns.lineplot(data=cost_month_source, x='dt_month', y='cac_source',\n",
    "                  hue = 'source_id', linewidth=1, palette=sns.color_palette(\"bright\", 7))\n",
    "ax.grid(axis='y')\n",
    "\n",
    "#Plot info\n",
    "ax.set_title('CAC of different sources in time')\n",
    "ax.set_xlabel('Months')\n",
    "ax.set_ylabel('USD')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's also calculate overall average CAC per source\n",
    "\n",
    "cac_source_mean = (cost_month_source.groupby('source_id')['cac_source'].mean()\n",
    "                   .sort_values(ascending = False).reset_index()\n",
    "                  )\n",
    "cac_source_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average CAC is the most expensive for source 3, it has high peak in August, and dropped greatly in May. \n",
    "\n",
    "The same dynamic have sources 2,1,5,4 and they are more stable in this period of time. \n",
    "\n",
    "Source 9 have the other dynamics, it increase in May to 8.5 usd. \n",
    "\n",
    "Very unstable CAC for source 10. Average CAC is 4.9 usd and it changes in range from 3 to 8 usd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How worthwhile where the investments? (ROI)\n",
    "ROI is the percentage ratio between income and investment in a business. The higher is the percentage, the more effective is the investment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's start with ROI per cohort.\n",
    "costs_month_roi = costs_month[['dt_month', 'cac']]\n",
    "#Renaming column `dt_month` in order to merge\n",
    "costs_month_roi.columns=['first_buy_month', 'cac']\n",
    "\n",
    "costs_month_roi['first_buy_month']=costs_month_roi['first_buy_month'].astype('datetime64[M]')\n",
    "\n",
    "#merging the tables costs_month_roi and ltv_cohort\n",
    "roi = ltv_cohort.merge(costs_month_roi, on = 'first_buy_month')\n",
    "\n",
    "#Check\n",
    "roi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate ROI\n",
    "roi['roi'] = roi.ltv / roi.cac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pivot of roi\n",
    "roi_pivot = roi.pivot_table(index='first_buy_month', columns = 'age', values = 'roi', aggfunc = 'mean'\n",
    "                           ).cumsum(axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the heatmap of ROI\n",
    "sns.set(style='dark')\n",
    "plt.figure(figsize=(11,6))\n",
    "sns.heatmap(roi_pivot, annot=True, fmt='.2f', linewidths=1, vmax=1.4, linecolor='gray')\n",
    "plt.title('Month Cohorts: ROI')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that only 2 cohorts are partly profitable. First June 2017 cohort is payed off from the 7th month so in December 2017 and finally it has 33% of gross profit for a year.\n",
    "\n",
    "The most profitable is September 2017 cohort with the biggest check. It is profitable from the 4th month and it has gross profit of 42%. Costs for all other cohorts are bigger than revenue. So we would recommend to increase marketing costs as there is a correlation between those costs and retention rate that we have determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's find ROI per source\n",
    "\n",
    "#Group orders to find revenue per source\n",
    "ltv_source = orders.groupby(['source_id'])['Uid', 'revenue'].agg({'Uid': 'nunique', 'revenue' :'sum'}).reset_index()\n",
    "#Renaming the columns\n",
    "ltv_source.rename(columns = {'Uid':'n_buyers'}, inplace = True)\n",
    "\n",
    "#ltv\n",
    "ltv_source['ltv_source'] = ltv_source.revenue / ltv_source.n_buyers\n",
    "\n",
    "#Check\n",
    "ltv_source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging ltv_source and costs_d tables with `source_id` column\n",
    "roi_source = ltv_source.merge(costs_d, on = 'source_id')\n",
    "\n",
    "#Calculating CAC per source \n",
    "roi_source['cac_source'] = roi_source.costs / roi_source.n_buyers\n",
    "\n",
    "#Calculating ROI\n",
    "roi_source['roi'] = roi_source.ltv_source / roi_source.cac_source\n",
    " \n",
    "#Checking the result\n",
    "roi_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting bar graph\n",
    "plt.figure(figsize = (10,6))\n",
    "ax = sns.barplot(data=roi_source, x='source_id', y='roi',linewidth=1, palette=sns.color_palette(\"bright\", 1))\n",
    "ax.grid(axis='y')\n",
    "plt.axhline(y=1, linewidth=3, color = 'black', alpha = 0.7, linestyle = '--')\n",
    "\n",
    "#Plotting additional info\n",
    "ax.set_title('ROI of different sources')\n",
    "ax.set_xlabel('Source')\n",
    "ax.set_ylabel('roi coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROI per source. \n",
    "\n",
    "The profitable sources are only 1,2 5,9. The biggest most expensive source number 3 is ruinously unprofitable, only about 40% of marketing costs are payed off. So should advise to redistribute marketing budjet in profitable sources in order to get more profitable users and stop burning money in source 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yandex.Afisha task was to help optimize marketing expenses. \n",
    "\n",
    "We had server logs with data on Yandex.Afisha visits from June 2017 through May 2018, dump file with all orders for the period and marketing expenses statistics. \n",
    "\n",
    "Purpose of the analysis:\n",
    "\n",
    "find out how people use the product, when they start to buy, how much money each customer brings to the company, when they pay off.\n",
    "\n",
    "We got data from 3 tables: visits, orders and costs. There were no missing values we just changed some column names to lowercase with the use of underscore between the words, we changed type of time stamp columns in order to work with the dates properly. We optimize type of device to category type.\n",
    "\n",
    "Yandex.Afisha has 907 users, who use the product every day in average. \n",
    "Average number of weekly users - 5716\n",
    "Average number of monthly users - 23228. \n",
    "16% of the users who have used the product at least once a week, continue to do it every day, and only 4% of the users who have used the product at least once a month, continue to use it every day. \n",
    "\n",
    "25% of users who have used the service once a month us it weekly. We saw that users open the product just once a day. Most often users spend just 1 minute using the product, but the half of the total number of users spend more than 6 minutes during the one session.\n",
    "\n",
    "10% of users continue using the product after the first week when they began to use it. The September 2017 Cohort has the highest Retention rate in the first month, but it dropped. In 3 months it drops to 4%. The most successful cohort is the first one June 2017 Cohort. They have 7.9% of users who continue to use the site in the next month after the first usage and the have this retention rate for the next 6 months. Then after 6 months retention rate begin to drop to 4.5% in May 2018. We see also that from December 2017 and in 2018 retention rate begins to drop for those cohorts below 6% in the first lifetime month.\n",
    "\n",
    "Clients usually order something in the same day. 75% of people make the first order within 4 days. But only 16% of users made at least one order.\n",
    "\n",
    "We discovered that our first cohort has the biggest number of orders per user per month. It has 2.19 orders per user in total. \n",
    "Average order size is 7.73 usd, Yandex.Afisha has the same average check in first cohort (6.85)\n",
    "The September Cohort has the maximum average check of 17.29 usd. The December2017 cohort has the biggest check of 14 usd. \n",
    "Average check is growing for the first June cohort, But in last month it dropped to average value.\n",
    "\n",
    "The September and December cohorts have the biggest average check in March (62.57 and 26.08 usd accordingly), then it decreases.\n",
    "\n",
    "The best revenue gives September Cohort. Each user from this cohort gives Yandex.Afisha 13.44 usd. \n",
    "\n",
    "Marketing\n",
    "The most expensive is the source number 3. Yandex.Afisha spent more than 140 000 usd for acquisition through that source. \n",
    "Marketing costs grew up till the end of 2017 year, after that the began to decrease. We have already notices that retention rate began to increase in that period of time. So this could be a result of spending more money on users acquisition. So there is a correlation between decreasing marketing costs and  decreasing retention rate.\n",
    "\n",
    "The Mean CAC per user is about 9 USD. This value changes significantly over time. For example, it reaches a maximum value in August 2017 of more than 10 and a minimum in May 2018 of more than 7. In general, acquisition seems prohibitively expensive, given that Yandex itself is engaged in contextual advertising. We need to think about properly built SEO optimization of this site, although this is beyond the scope of our study, since the sources of attraction for us are marked only with numbers.The most expensive is the source number 3. Average CAC is 13.8 usd for it and this level of costs is mostly constant. We have only high peak in August, but then CAC dropped to the mean level and we have great decrease in May 2018 where CAC dropped to 11 usd per source. We have the same dynamic with source 2 which have an average CAC of 12.5 USD. The same dynamic have sources 1,5,4 and thy are more stable in time, but for Source 9 we have the other dynamics: stable level about 5.4 usd and sudden increase in May to 8.5 usd. We have very unstable CAC for source 10. Average CAC of that source is 4.9 usd but it changes in range from 3 to 8 usd.\n",
    "\n",
    "Durin ROI analysis we saw that only 2 cohorts are payed off. Our first June cohort is payed off from the 7th month so in December 2017 and finally it has 33% of gross profit for a year. The most profitable is September cohort where was the biggest average check. It is profitable from the 4th month and it has gross profit of 42% for the whole analysis period. Costs for all other cohorts are bigger than profit. So we would recommend to increase marketing costs as there is a correlation between those costs and retention rate that we have determined during the analysis.\n",
    "\n",
    "ROI per source. The profitable sources are only 1,2 5,9. The biggest most expensive source number 3 is ruinously unprofitable, only about 40% of marketing costs are payed off. So should advise to redistribute marketing budjet in profitable sources in order to get more profitable users and stop burning money in source 3. \n",
    "\n",
    "Unfortunately,  marketing is not ok, because only 3 ad sources and 2 cohorts are payed off and profitable. There is such correlation that the more money we spend to users acquisition the more users we get. We found out that marketing expenses began to decrease in 2018, and this result in decrease in number of users. \n",
    "We recommend to increase investments into marketing, but change the ad source №3 to the profitable sources 1, 2 and 9."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
